syntax = "proto3";

package proto;

enum ReturnType {
    UNKNOWN                 = 0;
//    NODNS                   = 1;
    NOCONNECTION            = 2;
//    FORBIDDENROBOTS         = 3;
    TIMEOUT                 = 4;
    BADTYPE                 = 5;
    TOOBIG                  = 6;
    BADHEADER               = 7;
//    NETWORKERROR            = 8;
    SITEQUEUEFULLFETCHER    = 9;
    // url format is not avaliable  crawl/base/utils.IsInvalidUrl()
    INVALIDURL              = 10;
    INVALIDREDIRECTURL      = 11;
//    META_REDIRECT           = 12;
//    JS_REDIRECT             = 13;
//    IP_BLACKLISTED          = 14;
//    BADCONTENT              = 15;
//    URL_BLACKLISTED         = 16;
    SITEQUEUEFULLDISPATCHER = 17;

    STATUS100 = 100;
    STATUS101 = 101;

    STATUS200 = 200;
    STATUS201 = 201;
    STATUS202 = 202;
    STATUS203 = 203;
    STATUS204 = 204;
    STATUS205 = 205;
    STATUS206 = 206;

    STATUS300 = 300;
    STATUS301 = 301;
    STATUS302 = 302;
    STATUS303 = 303;
    STATUS304 = 304;
    STATUS305 = 305;
    STATUS306 = 306;
    STATUS307 = 307;

    STATUS400 = 400;
    STATUS401 = 401;
    STATUS402 = 402;
    STATUS403 = 403;
    STATUS404 = 404;
    STATUS405 = 405;
    STATUS406 = 406;
    STATUS407 = 407;
    STATUS408 = 408;
    STATUS409 = 409;
    STATUS410 = 410;
    STATUS411 = 411;
    STATUS412 = 412;
    STATUS413 = 413;
    STATUS414 = 414;
    STATUS415 = 415;
    STATUS416 = 416;
    STATUS417 = 417;

    STATUS500 = 500;
    STATUS501 = 501;
    STATUS502 = 502;
    STATUS503 = 503;
    STATUS504 = 504;
    STATUS505 = 505;
    STATUS509 = 509;
    STATUS510 = 510;
}

// tag for source. you can custom
// like primary_tag and second_tag
enum RequestType {
    TESTING     = 0;
    GENERAL     = 1;
}

enum Priority {
    NORMAL  =   0;
    URGENT  =   1;
}

// generate from page analysis
enum DocType {
    NORMALDOC      =   0;
    WEB_MAIN    =   1;
    WEB_HUB     =   2;
    WEB_CONTENT =   3;
}

message ConnectionInfo {
    string host = 1;
    int32  port = 2;
}

message OutLink {
    string  url  = 1;
    string  text = 2;
}

message FetchHint {
    string host =   1;
    string path =   2;
    // int32   ip  =   3;
    // int32   port    =   4;
    // string post_data = 5;
}

message CrawlRecord {
    int64   request_time    = 1;
    ConnectionInfo fetcher  = 2;
    // time the doc fetched
    int64   fetch_time      = 3;
    int64   fetch_use       = 4;
}

message CrawlParam {
    Priority    pri         =   1;
    int32       hostload    =   2;
    int32   random_hostload =   3;
    int32   fetcher_count   =   4;  // for multi fetcher, default should set 1, come from config file.
    bool    drop_content    =   5;
    bool    nofollow        =   6;

    string  store_engine    =   7;
    string  store_db        =   8;
    string  store_table     =   9;

    string  fake_host       =   10;  // filll from config file
    FetchHint fetch_hint    =   11;
    repeated ConnectionInfo receivers   = 12; // fill from config file

    string referer = 20;
    bool    custom_ua   =   21;
    bool    use_proxy   =   22;
    bool    follow_redirect =   23;

    RequestType rtype       =   101;
    string primary_tag = 102;
    repeated string secondary_tag = 103;
}

message CrawlHistory {
    // TODO.
}

message CrawlDoc {
    uint32  docid = 1;
    string  request_url = 2;
    // url use to crawl. it's generated base on request url
    string  url = 3;

    // fill at fetcher, http response information
    string redirect_url = 4;
    ReturnType  code    =   5;
    // record error.Errors.
    string error_info   =   6;
    string  content =   7;
    int64   content_length = 8;
    // compress at storage handler.
    bool    content_compressed  = 9;
    // response header.
    string  header  =   10;
    string last_modify = 11;
    // content type of the page, eg text/html
    string content_type = 12;


    repeated OutLink    indomain_outlinks = 30;
    repeated OutLink    outdomain_outlinks = 31;
    // content hash(128 bit)
    // int64 chash_0   =   32;
    // int64 chash_1   =   33;
    // original encoding which is deteched by the page content
    string orig_encoding   = 34;
    // encoding after convert to utf8
    // the same with orig_encoding if convert fail
    // utf8 if convert success
    string conv_encoding =   35;
    // language the page is detected to
    //int32 language = 36;

    string reservation = 40;

    CrawlParam  crawl_param = 50;
    CrawlRecord crawl_record = 60;
}
message CrawlDocs {
    repeated CrawlDoc docs = 1;
}
message CrawlRequest {
    string request = 1;
}
message CrawlResponse {
    bool ok = 1;
    int64   ret = 2;
}
service CrawlService {
    rpc Feed(CrawlDocs) returns (CrawlResponse) {}
    rpc IsHealthy(CrawlRequest) returns (CrawlResponse) {}
}
